{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "choice-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from tools.settings import *\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tools.train_val_test_spliter import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "neural-adoption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the ginen dataset into Train Test=0.3 Validation=0.2\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# To split dataset if already splits folder already exits no need to run it\n",
    "split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stainless-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd. read_csv(os.path.join(dataset_path, \"train.csv\"))\n",
    "test = pd. read_csv(os.path.join(dataset_path, \"test.csv\"))\n",
    "val = pd. read_csv(os.path.join(dataset_path, \"val.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pressing-hunger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video_url</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>slap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Video_url action\n",
       "0  H:/GitHub/real-time-action-recognition-from-vi...  punch\n",
       "1  H:/GitHub/real-time-action-recognition-from-vi...   kick\n",
       "2  H:/GitHub/real-time-action-recognition-from-vi...  punch\n",
       "3  H:/GitHub/real-time-action-recognition-from-vi...  punch\n",
       "4  H:/GitHub/real-time-action-recognition-from-vi...   slap"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tutorial-location",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video_url</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>punch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Video_url action\n",
       "0  H:/GitHub/real-time-action-recognition-from-vi...  punch\n",
       "1  H:/GitHub/real-time-action-recognition-from-vi...  punch\n",
       "2  H:/GitHub/real-time-action-recognition-from-vi...  punch\n",
       "3  H:/GitHub/real-time-action-recognition-from-vi...   kick\n",
       "4  H:/GitHub/real-time-action-recognition-from-vi...  punch"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adopted-dress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video_url</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>slap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>slap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Video_url action\n",
       "0  H:/GitHub/real-time-action-recognition-from-vi...  punch\n",
       "1  H:/GitHub/real-time-action-recognition-from-vi...  punch\n",
       "2  H:/GitHub/real-time-action-recognition-from-vi...   slap\n",
       "3  H:/GitHub/real-time-action-recognition-from-vi...  punch\n",
       "4  H:/GitHub/real-time-action-recognition-from-vi...   slap"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "biblical-brave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78, 2)\n",
      "(44, 2)\n",
      "(29, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "atlantic-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_frame(data, folder_name):\n",
    "    '''\n",
    "    Generated filenames format dataset_path/folder_name/video_name_frame{number}_action.jpg\n",
    "    '''\n",
    "    directory = os.path.join(dataset_path, folder_name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    for i in tqdm(range(data.shape[0])):\n",
    "        video_file = data['Video_url'][i]\n",
    "        action = data['action'][i]\n",
    "        video_name_list = video_file.split('/')[-1].split('.')\n",
    "        video_name_list = video_name_list[:-1]\n",
    "        video_name = \"\"\n",
    "        for n in video_name_list:\n",
    "            video_name += n\n",
    "        # capturing the video from the given path\n",
    "        capture = cv2.VideoCapture(video_file) \n",
    "        #frame rate\n",
    "        frame_rate = capture.get(5)\n",
    "        count = 0\n",
    "        while(capture.isOpened()):\n",
    "            #current frame number\n",
    "            frame_id = capture.get(1) \n",
    "            read_correctly, frame = capture.read()\n",
    "            if not read_correctly:\n",
    "                break\n",
    "            if (frame_id % math.floor(frame_rate) == 0):\n",
    "                # storing the frames in a new folder named train_1\n",
    "                filename = directory + \"/\" + video_name + \"_frame{}_\".format(count) + action +\".jpg\"\n",
    "                count += 1\n",
    "                cv2.imwrite(filename, frame)\n",
    "        capture.release()\n",
    "    print(\"Successfully Converted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "economic-invite",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [01:04<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Converted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "convert_to_frame(train, train_frames_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "incoming-peace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:23<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Converted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "convert_to_frame(val, val_frames_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "built-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paths_csv(directory, file_name):\n",
    "    images = os.listdir(directory)\n",
    "    images_path_list = []\n",
    "    images_action_list = [] \n",
    "    for image in images:\n",
    "        images_path_list.append(directory + image)\n",
    "        images_action_list.append(image.split('.')[0].split('_')[-1])\n",
    "    df = pd.DataFrame()\n",
    "    df['image'] = images_path_list\n",
    "    df['action'] = images_action_list\n",
    "    print(os.path.join(dataset_path, file_name+'.csv'))\n",
    "    df.to_csv(os.path.join(dataset_path, file_name+'.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "narrative-yacht",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:/GitHub/real-time-action-recognition-from-video-footage/dataset\\train_frames.csv\n"
     ]
    }
   ],
   "source": [
    "create_paths_csv(train_frames_path, train_frames_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "humanitarian-homeless",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:/GitHub/real-time-action-recognition-from-video-footage/dataset\\val_frames.csv\n"
     ]
    }
   ],
   "source": [
    "create_paths_csv(val_frames_path, val_frames_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beneficial-circular",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image action\n",
       "0  H:/GitHub/real-time-action-recognition-from-vi...   kick\n",
       "1  H:/GitHub/real-time-action-recognition-from-vi...   kick\n",
       "2  H:/GitHub/real-time-action-recognition-from-vi...   kick\n",
       "3  H:/GitHub/real-time-action-recognition-from-vi...   kick\n",
       "4  H:/GitHub/real-time-action-recognition-from-vi...   kick"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image = pd.read_csv(os.path.join(dataset_path, 'train_frames.csv'))\n",
    "train_image.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "rapid-harassment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(423, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "false-vietnam",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image action\n",
       "0  H:/GitHub/real-time-action-recognition-from-vi...   kick\n",
       "1  H:/GitHub/real-time-action-recognition-from-vi...   kick\n",
       "2  H:/GitHub/real-time-action-recognition-from-vi...   kick\n",
       "3  H:/GitHub/real-time-action-recognition-from-vi...   kick\n",
       "4  H:/GitHub/real-time-action-recognition-from-vi...   kick"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_image = pd.read_csv(os.path.join(dataset_path, 'val_frames.csv'))\n",
    "val_image.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "descending-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(161, 2)\n"
     ]
    }
   ],
   "source": [
    "print(val_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "tracked-sullivan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kick', 'punch', 'slap']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_values = list(train_image['action'].unique())\n",
    "action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "rolled-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_columns(df):\n",
    "    for value in action_values:\n",
    "        df[value] = np.where(df['action'].str.contains(value), 1, 0)\n",
    "    df.drop('action', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "muslim-richmond",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>kick</th>\n",
       "      <th>punch</th>\n",
       "      <th>slap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  kick  punch  slap\n",
       "0  H:/GitHub/real-time-action-recognition-from-vi...     1      0     0\n",
       "1  H:/GitHub/real-time-action-recognition-from-vi...     1      0     0\n",
       "2  H:/GitHub/real-time-action-recognition-from-vi...     1      0     0\n",
       "3  H:/GitHub/real-time-action-recognition-from-vi...     1      0     0\n",
       "4  H:/GitHub/real-time-action-recognition-from-vi...     1      0     0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_class_columns(train_image)\n",
    "train_image.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "occupational-clone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>kick</th>\n",
       "      <th>punch</th>\n",
       "      <th>slap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H:/GitHub/real-time-action-recognition-from-vi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  kick  punch  slap\n",
       "0  H:/GitHub/real-time-action-recognition-from-vi...     1      0     0\n",
       "1  H:/GitHub/real-time-action-recognition-from-vi...     1      0     0\n",
       "2  H:/GitHub/real-time-action-recognition-from-vi...     1      0     0\n",
       "3  H:/GitHub/real-time-action-recognition-from-vi...     1      0     0\n",
       "4  H:/GitHub/real-time-action-recognition-from-vi...     1      0     0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_class_columns(val_image)\n",
    "val_image.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "thirty-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_array_and_split(image_data):\n",
    "    image_value = []\n",
    "    for i in tqdm(range(image_data.shape[0])):\n",
    "        img = image.load_img(image_data['image'][i], target_size=(224,224,3))\n",
    "        img = image.img_to_array(img)\n",
    "        # normalizing the pixel value\n",
    "        img = img / 255\n",
    "        image_value.append(img)\n",
    "\n",
    "    X = np.array(image_value)\n",
    "    y = image_data\n",
    "    y.drop('image', axis='columns', inplace=True)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "mexican-holocaust",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 423/423 [00:04<00:00, 84.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(423, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = convert_to_array_and_split(train_image)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "selected-spyware",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 161/161 [00:01<00:00, 84.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(161, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "X_val, y_val = convert_to_array_and_split(val_image)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "married-animal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kick</th>\n",
       "      <th>punch</th>\n",
       "      <th>slap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kick  punch  slap\n",
       "0     1      0     0\n",
       "1     1      0     0\n",
       "2     1      0     0\n",
       "3     1      0     0\n",
       "4     1      0     0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cardiac-yugoslavia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kick</th>\n",
       "      <th>punch</th>\n",
       "      <th>slap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kick  punch  slap\n",
       "0     1      0     0\n",
       "1     1      0     0\n",
       "2     1      0     0\n",
       "3     1      0     0\n",
       "4     1      0     0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "interstate-default",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 28s 0us/step\n"
     ]
    }
   ],
   "source": [
    "'''This model was trained on a dataset that has 1,000 classes. \n",
    "include_top = False will remove the last layer of this model so that we can tune it as per our need.\n",
    "'''\n",
    "base_model = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "golden-examination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(423, 7, 7, 512)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting features for training frames\n",
    "X_train = base_model.predict(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "therapeutic-palmer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161, 7, 7, 512)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = base_model.predict(X_val)\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "jewish-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the training as well as validation frames in single dimension\n",
    "X_train = X_train.reshape(423, 7*7*512)\n",
    "X_val = X_val.reshape(161, 7*7*512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "traditional-twelve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(423, 25088)\n",
      "(161, 25088)\n"
     ]
    }
   ],
   "source": [
    "# normalizing the pixel values\n",
    "max_pixel = X_train.max()\n",
    "X_train = X_train / max_pixel\n",
    "X_val = X_val / max_pixel\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "color-breed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 26,380,547\n",
      "Trainable params: 26,380,547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The input shape will be 25,088\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_shape=(25088,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "deluxe-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mcp_weight = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "turned-cliff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 4s 884ms/step - loss: 1.0788 - accuracy: 0.4326 - val_loss: 0.9324 - val_accuracy: 0.5590\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 1s 196ms/step - loss: 1.2061 - accuracy: 0.4610 - val_loss: 0.9641 - val_accuracy: 0.5590\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 1s 188ms/step - loss: 1.1072 - accuracy: 0.4704 - val_loss: 0.9579 - val_accuracy: 0.5590\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 1s 181ms/step - loss: 1.0766 - accuracy: 0.4870 - val_loss: 0.9592 - val_accuracy: 0.5590\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 4s 909ms/step - loss: 1.0330 - accuracy: 0.4941 - val_loss: 0.9307 - val_accuracy: 0.5466\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 4s 967ms/step - loss: 0.9523 - accuracy: 0.5508 - val_loss: 0.8782 - val_accuracy: 0.5652\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.8945 - accuracy: 0.5887 - val_loss: 0.8412 - val_accuracy: 0.5901\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 4s 952ms/step - loss: 0.8356 - accuracy: 0.5934 - val_loss: 0.7974 - val_accuracy: 0.6149\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 4s 935ms/step - loss: 0.7538 - accuracy: 0.6478 - val_loss: 0.7645 - val_accuracy: 0.6025\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 4s 931ms/step - loss: 0.6426 - accuracy: 0.7305 - val_loss: 0.7349 - val_accuracy: 0.6460\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 4s 995ms/step - loss: 0.5819 - accuracy: 0.7281 - val_loss: 0.7057 - val_accuracy: 0.6460\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.5305 - accuracy: 0.7636 - val_loss: 0.6713 - val_accuracy: 0.6832\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 1s 212ms/step - loss: 0.4489 - accuracy: 0.8085 - val_loss: 0.6781 - val_accuracy: 0.6832\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 4s 949ms/step - loss: 0.3749 - accuracy: 0.8534 - val_loss: 0.6552 - val_accuracy: 0.7267\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 1s 201ms/step - loss: 0.3150 - accuracy: 0.8747 - val_loss: 0.6721 - val_accuracy: 0.7578\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 4s 992ms/step - loss: 0.2687 - accuracy: 0.8960 - val_loss: 0.6459 - val_accuracy: 0.7516\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 1s 206ms/step - loss: 0.2212 - accuracy: 0.9291 - val_loss: 0.6733 - val_accuracy: 0.7516\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 1s 195ms/step - loss: 0.1728 - accuracy: 0.9433 - val_loss: 0.6476 - val_accuracy: 0.7640\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 1s 217ms/step - loss: 0.1655 - accuracy: 0.9456 - val_loss: 0.7405 - val_accuracy: 0.7578\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 1s 209ms/step - loss: 0.1262 - accuracy: 0.9622 - val_loss: 0.6959 - val_accuracy: 0.7764\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 1s 198ms/step - loss: 0.1109 - accuracy: 0.9598 - val_loss: 0.7581 - val_accuracy: 0.7516\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 1s 226ms/step - loss: 0.0984 - accuracy: 0.9622 - val_loss: 0.7252 - val_accuracy: 0.7764\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 1s 195ms/step - loss: 0.0892 - accuracy: 0.9693 - val_loss: 0.8712 - val_accuracy: 0.7640\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 1s 204ms/step - loss: 0.0731 - accuracy: 0.9764 - val_loss: 0.8280 - val_accuracy: 0.8012\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 1s 204ms/step - loss: 0.0666 - accuracy: 0.9811 - val_loss: 0.8827 - val_accuracy: 0.7578\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 1s 197ms/step - loss: 0.0406 - accuracy: 0.9929 - val_loss: 0.9502 - val_accuracy: 0.7702\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 1s 187ms/step - loss: 0.0467 - accuracy: 0.9882 - val_loss: 0.9007 - val_accuracy: 0.7888\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 1s 186ms/step - loss: 0.0462 - accuracy: 0.9858 - val_loss: 0.9232 - val_accuracy: 0.7764\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.0363 - accuracy: 0.9905 - val_loss: 1.0550 - val_accuracy: 0.7578\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 1s 192ms/step - loss: 0.0177 - accuracy: 0.9953 - val_loss: 0.9945 - val_accuracy: 0.7578\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 1s 213ms/step - loss: 0.0131 - accuracy: 0.9976 - val_loss: 1.0755 - val_accuracy: 0.7888\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 1s 217ms/step - loss: 0.0215 - accuracy: 0.9953 - val_loss: 1.1499 - val_accuracy: 0.7702\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 1s 185ms/step - loss: 0.0343 - accuracy: 0.9882 - val_loss: 1.0100 - val_accuracy: 0.7764\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 1s 184ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 1.1699 - val_accuracy: 0.7764\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 1s 207ms/step - loss: 0.0095 - accuracy: 0.9976 - val_loss: 1.0824 - val_accuracy: 0.7950\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 1s 212ms/step - loss: 0.0148 - accuracy: 0.9953 - val_loss: 1.0417 - val_accuracy: 0.7950\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 1s 207ms/step - loss: 0.0089 - accuracy: 0.9976 - val_loss: 1.0730 - val_accuracy: 0.7702\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 1s 191ms/step - loss: 0.0085 - accuracy: 0.9976 - val_loss: 1.1889 - val_accuracy: 0.7764\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 1s 226ms/step - loss: 0.0068 - accuracy: 0.9976 - val_loss: 1.1816 - val_accuracy: 0.7764\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 1s 236ms/step - loss: 0.0086 - accuracy: 0.9976 - val_loss: 1.1121 - val_accuracy: 0.7950\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 1s 238ms/step - loss: 0.0136 - accuracy: 0.9929 - val_loss: 1.1980 - val_accuracy: 0.7702\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 1s 207ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.6506 - val_accuracy: 0.7702\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 1s 221ms/step - loss: 0.0126 - accuracy: 0.9953 - val_loss: 1.2415 - val_accuracy: 0.7702\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 1s 224ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 1.2691 - val_accuracy: 0.7950\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 1s 205ms/step - loss: 0.0071 - accuracy: 0.9976 - val_loss: 1.2382 - val_accuracy: 0.7950\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 1s 254ms/step - loss: 0.0154 - accuracy: 0.9929 - val_loss: 1.3467 - val_accuracy: 0.7764\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 1s 212ms/step - loss: 0.0172 - accuracy: 0.9953 - val_loss: 1.2496 - val_accuracy: 0.7764\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 1s 155ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.2189 - val_accuracy: 0.7950\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 1s 220ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2240 - val_accuracy: 0.7826\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 1s 213ms/step - loss: 0.0134 - accuracy: 0.9953 - val_loss: 1.5934 - val_accuracy: 0.7764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x249814d64c0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[mcp_weight], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "minus-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weight.hdf5\")\n",
    "# model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "choice-angle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 44/44 [01:20<00:00,  1.82s/it]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats as s\n",
    "predict = []\n",
    "actual = []\n",
    "if not os.path.exists(test_frames_path):\n",
    "    os.makedirs(test_frames_path)\n",
    "\n",
    "for i in tqdm(range(test.shape[0])):\n",
    "    video_file = test['Video_url'][i]\n",
    "    action = test['action'][i]\n",
    "    video_name_list = video_file.split('/')[-1].split('.')\n",
    "    video_name_list = video_name_list[:-1]\n",
    "    video_name = \"\"\n",
    "    for n in video_name_list:\n",
    "        video_name += n\n",
    "    # capturing the video from the given path\n",
    "    capture = cv2.VideoCapture(video_file) \n",
    "    #frame rate\n",
    "    frame_rate = capture.get(5)\n",
    "    count = 0\n",
    "    files = glob(test_frames_path + '/*')\n",
    "    #removing all files from folder\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "    while(capture.isOpened()):\n",
    "        #current frame number\n",
    "        frame_id = capture.get(1) \n",
    "        read_correctly, frame = capture.read()\n",
    "        if not read_correctly:\n",
    "            break\n",
    "        if (frame_id % math.floor(frame_rate) == 0):\n",
    "            # storing the frames in a new folder named train_1\n",
    "            filename = test_frames_path + \"/\" + video_name + \"_frame{}_\".format(count) + action +\".jpg\"\n",
    "            count += 1\n",
    "            cv2.imwrite(filename, frame)\n",
    "    capture.release()\n",
    "    \n",
    "    # reading all the frames from temp folder\n",
    "    images = glob(test_frames_path + '/*.jpg')\n",
    "    prediction_images = []\n",
    "    for i in range(len(images)):\n",
    "        img = image.load_img(images[i], target_size=(224,224,3))\n",
    "        img = image.img_to_array(img)\n",
    "        img = img / 255\n",
    "        prediction_images.append(img)\n",
    "        \n",
    "    # converting all the frames for a test video into numpy array\n",
    "    prediction_images = np.array(prediction_images)\n",
    "    # extracting features using pre-trained model\n",
    "    prediction_images = base_model.predict(prediction_images)\n",
    "    # converting features in one dimensional array\n",
    "    prediction_images = prediction_images.reshape(prediction_images.shape[0], 7*7*512)\n",
    "    # predicting tags for each array\n",
    "    prediction = np.argmax(model.predict(prediction_images), axis=-1)\n",
    "    # appending the mode of predictions in predict list to assign the tag to the video\n",
    "    predict.append(y_train.columns.values[s.mode(prediction)[0][0]])\n",
    "    # appending the actual tag of the video\n",
    "    actual.append(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "understood-virus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kick'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.columns.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "featured-prince",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-3f69b14594fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(predict, actual)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "imperial-medline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punch slap\n",
      "slap slap\n",
      "kick punch\n",
      "kick kick\n",
      "slap slap\n",
      "slap slap\n",
      "slap slap\n",
      "kick kick\n",
      "punch punch\n",
      "slap slap\n",
      "slap slap\n",
      "slap slap\n",
      "punch punch\n",
      "slap slap\n",
      "slap slap\n",
      "kick kick\n",
      "kick kick\n",
      "slap slap\n",
      "punch punch\n",
      "punch punch\n",
      "slap slap\n",
      "punch slap\n",
      "punch punch\n",
      "slap slap\n",
      "slap slap\n",
      "kick kick\n",
      "slap slap\n",
      "slap slap\n",
      "punch punch\n",
      "kick kick\n",
      "slap slap\n",
      "punch punch\n",
      "kick kick\n",
      "slap slap\n",
      "punch punch\n",
      "slap slap\n",
      "punch punch\n",
      "slap slap\n",
      "slap slap\n",
      "punch punch\n",
      "slap slap\n",
      "punch punch\n",
      "punch punch\n",
      "kick kick\n",
      "punch punch\n",
      "kick kick\n",
      "kick kick\n",
      "punch punch\n",
      "punch punch\n",
      "kick kick\n",
      "punch punch\n",
      "slap slap\n",
      "slap slap\n",
      "slap slap\n",
      "slap slap\n",
      "slap slap\n",
      "punch punch\n",
      "slap slap\n",
      "slap punch\n",
      "punch punch\n",
      "punch punch\n",
      "kick kick\n",
      "kick punch\n",
      "punch punch\n",
      "punch slap\n",
      "slap slap\n",
      "slap slap\n",
      "slap slap\n",
      "punch punch\n",
      "kick kick\n",
      "punch punch\n",
      "slap slap\n",
      "punch punch\n",
      "slap slap\n",
      "kick kick\n",
      "kick kick\n",
      "punch kick\n",
      "punch punch\n",
      "punch punch\n",
      "slap slap\n",
      "kick kick\n",
      "kick kick\n",
      "kick kick\n",
      "kick kick\n",
      "kick kick\n",
      "kick kick\n",
      "slap slap\n",
      "slap slap\n",
      "kick kick\n",
      "kick kick\n",
      "slap slap\n",
      "slap slap\n",
      "slap slap\n",
      "punch punch\n",
      "slap slap\n",
      "kick kick\n",
      "punch punch\n",
      "slap slap\n",
      "slap slap\n",
      "punch punch\n",
      "slap slap\n",
      "punch punch\n",
      "slap slap\n",
      "slap slap\n",
      "kick kick\n",
      "punch punch\n",
      "punch punch\n",
      "slap slap\n",
      "punch punch\n",
      "slap kick\n",
      "kick kick\n"
     ]
    }
   ],
   "source": [
    "# for i in range(0, len(predict)):\n",
    "#     print(predict[i] + \" \" + actual[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-rwanda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
